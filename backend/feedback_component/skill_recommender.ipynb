{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import copy\n",
    "from skill_classifier import SkillClassifier\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LM Studio and MongoDb configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LMStudio CONFIG ---\n",
    "API_URL = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"jobs_data\"]\n",
    "collection = db[\"jobs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from MongoDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching job data from MongoDB...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching job data from MongoDB...\")\n",
    "data = list(collection.find(\n",
    "    {\"core_position\": {\"$exists\": True}, \"core_skills\": {\"$exists\": True, \"$ne\": []}},\n",
    "    {\"core_position\": 1, \"core_skills\": 1, \"text\": 1, \"_id\": 0}\n",
    "))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning skill lists...\n",
      "Filtering rare skills...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'feedback_component'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33mfiltered_skills\u001b[39m\u001b[33m\"\u001b[39m].str.len() > \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Drop rows with no common skills\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# df[\"filtered_skills\"] = df[\"filtered_skills\"].apply(ast.literal_eval)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeedback_component/cleaned_job_data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ai-resume-parser\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: 'feedback_component'"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning skill lists...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def clean_skills(skill_list):\n",
    "    if not isinstance(skill_list, list):\n",
    "        return []\n",
    "    cleaned_skills = []\n",
    "    for skill in skill_list:\n",
    "        if isinstance(skill, str):\n",
    "            doc = nlp(skill.lower().strip())\n",
    "            for token in doc:\n",
    "                if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                    cleaned_skills.append(token.text)\n",
    "    return cleaned_skills\n",
    "\n",
    "df[\"core_skills_cleaned\"] = df[\"core_skills\"].apply(clean_skills)\n",
    "\n",
    "# --- Filter rare skills (keep skills used in ≥5 jobs) ---\n",
    "print(\"Filtering rare skills...\")\n",
    "flat_skills = [s for sublist in df[\"core_skills_cleaned\"] for s in sublist]\n",
    "skill_counts = Counter(flat_skills)\n",
    "common_skills = set([s for s, count in skill_counts.items() if count >= 5])\n",
    "\n",
    "df[\"filtered_skills\"] = df[\"core_skills_cleaned\"].apply(lambda skills: [s for s in skills if s in common_skills])\n",
    "df = df[df[\"filtered_skills\"].str.len() > 0]  # Drop rows with no common skills\n",
    "# df[\"filtered_skills\"] = df[\"filtered_skills\"].apply(ast.literal_eval)\n",
    "df.to_csv(\"models/cleaned_job_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"models/cleaned_job_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"models/cleaned_job_data.csv\")\n",
    "df[\"filtered_skills\"] = df[\"filtered_skills\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed job titles with descriptions using LM Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Embed job titles using LM Studio ---\n",
    "def get_embedding(text):\n",
    "    payload = {\n",
    "        \"input\": text,\n",
    "        \"model\": \"text-embedding-nomic-embed-text-v1.5\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload).json()\n",
    "        return np.array(response[\"data\"][0][\"embedding\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding '{text}': {e}\")\n",
    "        return np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "def get_avg_embedding(text, chunk_size=300):\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(\n",
    "    df.apply(lambda row: get_avg_embedding(f\"{row['core_position']}. {row['text']}\"), axis=1)\n",
    ")\n",
    "with open(\"models/job_title_embeddings.npy\", \"wb\") as f:\n",
    "    np.save(f, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting skills...\n",
      "Generating embeddings for skills...\n",
      "Clustering skills...\n",
      "Creating mappping...\n"
     ]
    }
   ],
   "source": [
    "# Flatten and count\n",
    "print(\"Counting skills...\")\n",
    "flat_skills = [skill for sublist in df[\"filtered_skills\"] for skill in sublist]\n",
    "skill_counts = Counter(flat_skills)\n",
    "\n",
    "# Get top N skills (optional)\n",
    "all_skills = [s for s, count in skill_counts.items()]\n",
    "\n",
    "print(\"Generating embeddings for skills...\")\n",
    "sentence_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "skill_embeddings = sentence_transformer.encode(all_skills)\n",
    "\n",
    "print(\"Clustering skills...\")\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=2, metric=\"cosine\")\n",
    "labels = dbscan.fit_predict(skill_embeddings)\n",
    "\n",
    "# Create mapping\n",
    "print(\"Creating mappping...\")\n",
    "skill_cluster_df = pd.DataFrame({\"skill\": all_skills, \"cluster\": labels})\n",
    "\n",
    "canonical_map = {}\n",
    "grouped = skill_cluster_df.groupby(\"cluster\")[\"skill\"]\n",
    "\n",
    "for cluster_id, group in grouped:\n",
    "    if cluster_id == -1:  # noise, skip or keep as-is\n",
    "        for skill in group:\n",
    "            canonical_map[skill] = skill\n",
    "    else:\n",
    "        # Use most frequent term or the first one\n",
    "        canonical_skill = group.iloc[0]\n",
    "        for skill in group:\n",
    "            canonical_map[skill] = canonical_skill\n",
    "\n",
    "standardized_skills = []\n",
    "for skill_list in df[\"filtered_skills\"]:\n",
    "    clean = [canonical_map.get(s, s) for s in skill_list]\n",
    "    standardized_skills.append(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label encoding skills...\n",
      "Loaded skill label count: 3039\n",
      "['#' '%' '+' ... 'zoo' 'zoology' 'zoom']\n",
      "✅ Saved skill label binarizer with 3039 skills\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# --- Multi-label encode skills ---\n",
    "print(\"Multi-label encoding skills...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(standardized_skills)\n",
    "print(\"Loaded skill label count:\", len(mlb.classes_))\n",
    "\n",
    "# Save the encoded skills matrix Y\n",
    "with open(\"models/skill_encoded_matrix.npy\", \"wb\") as f:\n",
    "    np.save(f, Y)\n",
    "    \n",
    "joblib.dump(mlb, 'models/skill_label_binarizer.joblib')\n",
    "\n",
    "print(mlb.classes_)\n",
    "print(\"✅ Saved skill label binarizer with\", len(mlb.classes_), \"skills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"models/job_title_embeddings.npy\")\n",
    "Y = np.load(\"models/skill_encoded_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (33042, 768)\n",
      "Y shape: (33042, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping triggered at epoch 11.\n",
      "✅ Best F1 (Top-K=5): 0.2333\n",
      "📦 Model saved to models/skill_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Convert data\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tensor, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, Y_train = X_train.to(device), Y_train.to(device)\n",
    "\n",
    "skill_classifier_model = SkillClassifier(X.shape[1], Y.shape[1]).to(device)\n",
    "\n",
    "# Train\n",
    "optimizer = optim.AdamW(skill_classifier_model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience = 5\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(50):\n",
    "    skill_classifier_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = skill_classifier_model(X_train)\n",
    "    loss = loss_fn(outputs, Y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation F1 (Top-K = 5 by default for early stopping)\n",
    "    skill_classifier_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = skill_classifier_model(X_test)\n",
    "        preds = torch.zeros_like(val_outputs)\n",
    "        topk_indices = torch.topk(val_outputs, 5, dim=1).indices\n",
    "        for i, row in enumerate(topk_indices):\n",
    "            preds[i, row] = 1\n",
    "\n",
    "    f1 = f1_score(Y_test.cpu(), preds.cpu(), average=\"micro\")\n",
    "\n",
    "    # Early stopping\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        wait = 0\n",
    "        best_model_state = copy.deepcopy(skill_classifier_model.state_dict())\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"⏹️ Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "print(f\"✅ Best F1 (Top-K=5): {best_f1:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "model_path = f\"models/skill_model.pth\"\n",
    "torch.save(best_model_state, model_path)\n",
    "print(f\"📦 Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkillClassifier(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.2, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=3039, bias=True)\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model for final Top-K evaluations\n",
    "model_path = f\"models/skill_model.pth\"\n",
    "skill_classifier_model.load_state_dict(torch.load(model_path))\n",
    "skill_classifier_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Top K: 3\n",
      "F1 (micro): 0.1718\n",
      "Precision: 0.6321\n",
      "Recall: 0.0994\n",
      "\n",
      "📊 Top K: 5\n",
      "F1 (micro): 0.2329\n",
      "Precision: 0.5609\n",
      "Recall: 0.1470\n"
     ]
    }
   ],
   "source": [
    "for top_k in [3, 5]:\n",
    "    with torch.no_grad():\n",
    "        outputs = skill_classifier_model(X_test)\n",
    "        preds = torch.zeros_like(outputs)\n",
    "        topk_indices = torch.topk(outputs, top_k, dim=1).indices\n",
    "        for i, row in enumerate(topk_indices):\n",
    "            preds[i, row] = 1\n",
    "\n",
    "    f1 = f1_score(Y_test.cpu(), preds.cpu(), average=\"micro\")\n",
    "    precision = precision_score(Y_test.cpu(), preds.cpu(), average=\"micro\")\n",
    "    recall = recall_score(Y_test.cpu(), preds.cpu(), average=\"micro\")\n",
    "\n",
    "    print(f\"\\n📊 Top K: {top_k}\")\n",
    "    print(f\"F1 (micro): {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Suggested Skills: ['management', 'microsoft', 'service', 'time', 'customer']\n"
     ]
    }
   ],
   "source": [
    "def recommend_skills_from_summary(job_title: str, summary: str, top_k: int = 5):\n",
    "    # Combine and embed input\n",
    "    input_text = f\"{job_title}. {summary}\"\n",
    "    embedding = get_embedding(input_text)\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "\n",
    "    # Predict and extract top-k indices\n",
    "    with torch.no_grad():\n",
    "        outputs = skill_classifier_model(embedding_tensor)\n",
    "        topk_indices = torch.topk(outputs, top_k, dim=1).indices[0].cpu().numpy()\n",
    "\n",
    "    # Map indices back to skill labels\n",
    "    predicted_skills = [mlb.classes_[i] for i in topk_indices]\n",
    "    return predicted_skills\n",
    "\n",
    "job_title = \"Data Scientist\"\n",
    "summary = \"Experienced in analyzing large datasets, building machine learning models, and presenting insights to stakeholders.\"\n",
    "\n",
    "recommended_skills = recommend_skills_from_summary(job_title, summary, top_k=5)\n",
    "print(\"🔧 Suggested Skills:\", recommended_skills)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
